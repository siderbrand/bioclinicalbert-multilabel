{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3e1dcb",
      "metadata": {
        "id": "cc3e1dcb",
        "outputId": "291244b2-7768-4903-8250-18d61ffa6e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: directorios listos\n"
          ]
        }
      ],
      "source": [
        "import os, json, math, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, average_precision_score, roc_auc_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "SEED = 42\n",
        "rng = np.random.RandomState(SEED)\n",
        "\n",
        "# Paths\n",
        "DATA_DIR   = Path(\"../data\")\n",
        "MODELS_DIR = Path(\"../models\")\n",
        "EXP_DIR    = MODELS_DIR / \"experiments\"\n",
        "BASE_DIR   = MODELS_DIR / \"baseline\"\n",
        "REPORTS    = Path(\"../reports\")\n",
        "\n",
        "for d in [MODELS_DIR, EXP_DIR, BASE_DIR, REPORTS]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"OK: directorios listos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6b14d58",
      "metadata": {
        "id": "e6b14d58",
        "outputId": "d5dfeec6-cff4-45ff-a47a-b89649478d55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Splits listos → train: 2368 valid: 595 test: 600\n"
          ]
        }
      ],
      "source": [
        "# Resolver/crear splits desde:\n",
        "# keys = ['seed','test_idx','folds','classes']\n",
        "import numpy as np, json, math\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def parse_labels(s):\n",
        "    if isinstance(s, float) and math.isnan(s):\n",
        "        return []\n",
        "    return [x.strip() for x in str(s).split(\"|\") if x.strip()]\n",
        "\n",
        "splits_path = DATA_DIR / \"splits.json\"\n",
        "with open(splits_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw = json.load(f)\n",
        "\n",
        "idx_all  = np.arange(len(df))\n",
        "idx_test = np.array(raw.get(\"test_idx\", []), dtype=int)\n",
        "\n",
        "def extract_train_valid_from_folds(folds, idx_all, idx_test):\n",
        "    \"\"\"\n",
        "    Soporta:\n",
        "      - folds = [ {train/train_idx, valid/val/valid_idx/val_idx}, ... ]\n",
        "      - folds = [ valid_idx_list, valid_idx_list, ... ]\n",
        "    Usa el primer fold para construir train/valid.\n",
        "    \"\"\"\n",
        "    if not isinstance(folds, list) or len(folds) == 0:\n",
        "        return None, None\n",
        "\n",
        "    f0 = folds[0]\n",
        "\n",
        "    # Caso A: dict con claves explícitas\n",
        "    if isinstance(f0, dict):\n",
        "        # intenta encontrar valid\n",
        "        val = (f0.get(\"valid\") or f0.get(\"val\") or\n",
        "               f0.get(\"valid_idx\") or f0.get(\"val_idx\"))\n",
        "        # intenta encontrar train\n",
        "        trn = (f0.get(\"train\") or f0.get(\"train_idx\") or\n",
        "               f0.get(\"train_indices\"))\n",
        "\n",
        "        if val is not None and trn is not None:\n",
        "            return np.array(trn, dtype=int), np.array(val, dtype=int)\n",
        "\n",
        "        if val is not None:\n",
        "            val = np.array(val, dtype=int)\n",
        "            mask = np.ones(len(idx_all), dtype=bool)\n",
        "            mask[val] = False\n",
        "            if idx_test.size > 0:\n",
        "                mask[idx_test] = False\n",
        "            trn = idx_all[mask]\n",
        "            return trn, val\n",
        "\n",
        "        if trn is not None:\n",
        "            trn = np.array(trn, dtype=int)\n",
        "            # si solo hay train, definimos valid como el complemento (excluyendo test)\n",
        "            mask = np.ones(len(idx_all), dtype=bool)\n",
        "            mask[trn] = False\n",
        "            if idx_test.size > 0:\n",
        "                mask[idx_test] = False\n",
        "            val = idx_all[mask]\n",
        "            return trn, val\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    # Caso B: lista/tupla = valid_idx directo\n",
        "    if isinstance(f0, (list, tuple, np.ndarray)):\n",
        "        val = np.array(f0, dtype=int)\n",
        "        mask = np.ones(len(idx_all), dtype=bool)\n",
        "        mask[val] = False\n",
        "        if idx_test.size > 0:\n",
        "            mask[idx_test] = False\n",
        "        trn = idx_all[mask]\n",
        "        return trn, val\n",
        "\n",
        "    return None, None\n",
        "\n",
        "idx_train, idx_valid = extract_train_valid_from_folds(raw.get(\"folds\", []), idx_all, idx_test)\n",
        "\n",
        "# Si algo faltó, generamos splits nuevos (estratificando por combinación de etiquetas),\n",
        "# pero respetando el test ya dado.\n",
        "if idx_train is None or idx_valid is None or idx_test.size == 0:\n",
        "    print(\"⚠️  No se pudo reconstruir train/valid desde 'folds' o falta 'test_idx'. Creando splits nuevos…\")\n",
        "    # excluye test de la población a dividir\n",
        "    pop = np.array([i for i in idx_all if i not in set(idx_test.tolist())])\n",
        "    # estratificación por combinación de etiquetas (aprox multilabel)\n",
        "    y_combo = ['|'.join(sorted(parse_labels(x))) or 'none' for x in df[label_col]]\n",
        "    y_pop = [y_combo[i] for i in pop]\n",
        "\n",
        "    idx_trn, idx_val = train_test_split(\n",
        "        pop, test_size=0.1765,  # ~ 15% valid si test ya es 15% (ajusta si quieres)\n",
        "        random_state=SEED, stratify=y_pop\n",
        "    )\n",
        "    idx_train, idx_valid = np.array(idx_trn, dtype=int), np.array(idx_val, dtype=int)\n",
        "    if idx_test.size == 0:\n",
        "        # si no había test, lo creamos (15% del total)\n",
        "        remaining = np.array([i for i in idx_all if i not in set(idx_train.tolist()+idx_valid.tolist())])\n",
        "        y_rem = [y_combo[i] for i in remaining]\n",
        "        _, idx_tst = train_test_split(remaining, test_size=0.5, random_state=SEED, stratify=y_rem)\n",
        "        idx_test = np.array(idx_tst, dtype=int)\n",
        "\n",
        "# Ordena por prolijidad (opcional)\n",
        "idx_train = np.sort(np.array(idx_train, dtype=int))\n",
        "idx_valid = np.sort(np.array(idx_valid, dtype=int))\n",
        "idx_test  = np.sort(np.array(idx_test,  dtype=int))\n",
        "\n",
        "# Persistimos ESTÁNDAR para el resto del pipeline\n",
        "with open(splits_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\n",
        "        \"train\": idx_train.tolist(),\n",
        "        \"valid\": idx_valid.tolist(),\n",
        "        \"test\":  idx_test.tolist()\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"Splits listos → train:\", len(idx_train), \"valid:\", len(idx_valid), \"test:\", len(idx_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "085f6adf",
      "metadata": {
        "id": "085f6adf",
        "outputId": "593e6cbb-27fb-420d-b637-1116a816e966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF guardado en ..\\models\\baseline\\tfidf.joblib\n",
            "Shapes: (2368, 28782) (595, 28782) (600, 28782)\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "BASE_DIR = Path(\"../models/baseline\"); BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer=\"word\", ngram_range=(1,2),\n",
        "    min_df=2, max_df=0.9, max_features=120_000,\n",
        "    lowercase=True, strip_accents=\"unicode\", sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train = tfidf.fit_transform([X_text[i] for i in idx_train])\n",
        "X_valid = tfidf.transform([X_text[i] for i in idx_valid])\n",
        "X_test  = tfidf.transform([X_text[i] for i in idx_test])\n",
        "\n",
        "Y_train = Y[idx_train]; Y_valid = Y[idx_valid]; Y_test = Y[idx_test]\n",
        "\n",
        "joblib.dump(tfidf, BASE_DIR / \"tfidf.joblib\")\n",
        "print(\"TF-IDF guardado en\", BASE_DIR / \"tfidf.joblib\")\n",
        "print(\"Shapes:\", X_train.shape, X_valid.shape, X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ced87c8",
      "metadata": {
        "id": "3ced87c8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, average_precision_score, roc_auc_score\n",
        "\n",
        "def to_proba(model, X):\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        p = model.predict_proba(X)\n",
        "        if isinstance(p, list): p = np.column_stack([col[:,1] for col in p])\n",
        "        return p.astype(\"float64\")\n",
        "    scores = model.decision_function(X).astype(\"float64\")\n",
        "    return 1.0 / (1.0 + np.exp(-scores))\n",
        "\n",
        "def safe_roc_auc(y_true, proba):\n",
        "    aucs=[]\n",
        "    for j in range(proba.shape[1]):\n",
        "        yj = y_true[:,j]\n",
        "        if yj.sum()>0 and (len(yj)-yj.sum())>0:\n",
        "            try: aucs.append(roc_auc_score(yj, proba[:,j]))\n",
        "            except: pass\n",
        "    return float(np.mean(aucs)) if aucs else float(\"nan\")\n",
        "\n",
        "def metrics_from_proba(y_true, proba, thr):\n",
        "    y_pred = (proba >= thr[None,:]).astype(int)\n",
        "    return {\n",
        "        \"f1_micro\": float(f1_score(y_true,y_pred,average=\"micro\",zero_division=0)),\n",
        "        \"f1_macro\": float(f1_score(y_true,y_pred,average=\"macro\",zero_division=0)),\n",
        "        \"pr_macro\": float(average_precision_score(y_true, proba, average=\"macro\")),\n",
        "        \"roc_macro\": float(safe_roc_auc(y_true, proba)),\n",
        "    }\n",
        "\n",
        "def find_thresholds_per_class(y_true, proba, grid=np.linspace(0.2,0.8,31)):\n",
        "    C=proba.shape[1]; best=np.zeros(C,dtype=\"float64\")\n",
        "    for j in range(C):\n",
        "        yj=y_true[:,j]\n",
        "        if yj.sum()==0: best[j]=0.5; continue\n",
        "        best_f1,best_t=-1.0,0.5\n",
        "        for t in grid:\n",
        "            f1j=f1_score(yj,(proba[:,j]>=t).astype(int),zero_division=0)\n",
        "            if f1j>best_f1: best_f1,best_t=f1j,t\n",
        "        best[j]=best_t\n",
        "    return best\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b936cf",
      "metadata": {
        "id": "44b936cf",
        "outputId": "a53622b4-e703-4073-a237-291f58376339"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['logreg_C2', 'ridge_a1', 'linsvm_C1_cal', 'cnb_a05']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "SEED=42; N_JOBS=-1; MAX_ITER_LR=5000\n",
        "\n",
        "def get_models():\n",
        "    return {\n",
        "        \"logreg_C2\": OneVsRestClassifier(\n",
        "            LogisticRegression(solver=\"saga\", penalty=\"l2\", C=2.0,\n",
        "                               max_iter=MAX_ITER_LR, n_jobs=N_JOBS,\n",
        "                               class_weight=\"balanced\", random_state=SEED),\n",
        "            n_jobs=N_JOBS),\n",
        "        \"ridge_a1\": OneVsRestClassifier(\n",
        "            RidgeClassifier(alpha=1.0, class_weight=\"balanced\", random_state=SEED),\n",
        "            n_jobs=N_JOBS),\n",
        "        \"linsvm_C1_cal\": OneVsRestClassifier(\n",
        "            CalibratedClassifierCV(LinearSVC(C=1.0, random_state=SEED),\n",
        "                                   method=\"sigmoid\", cv=5),\n",
        "            n_jobs=N_JOBS),\n",
        "        \"cnb_a05\": OneVsRestClassifier(ComplementNB(alpha=0.5), n_jobs=N_JOBS),\n",
        "    }\n",
        "\n",
        "MODELS=get_models(); list(MODELS.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82dd0372",
      "metadata": {
        "id": "82dd0372",
        "outputId": "a1e097d1-e831-4b26-9794-e8e716670c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando logreg_C2 ...\n",
            "Entrenando ridge_a1 ...\n",
            "Entrenando linsvm_C1_cal ...\n",
            "Entrenando cnb_a05 ...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1_micro</th>\n",
              "      <th>f1_macro</th>\n",
              "      <th>pr_macro</th>\n",
              "      <th>roc_macro</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.892269</td>\n",
              "      <td>0.884619</td>\n",
              "      <td>0.950481</td>\n",
              "      <td>0.969225</td>\n",
              "      <td>linsvm_C1_cal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.888318</td>\n",
              "      <td>0.878914</td>\n",
              "      <td>0.947031</td>\n",
              "      <td>0.967621</td>\n",
              "      <td>ridge_a1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.878353</td>\n",
              "      <td>0.870171</td>\n",
              "      <td>0.941549</td>\n",
              "      <td>0.963196</td>\n",
              "      <td>logreg_C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.810845</td>\n",
              "      <td>0.773244</td>\n",
              "      <td>0.842227</td>\n",
              "      <td>0.906894</td>\n",
              "      <td>cnb_a05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   f1_micro  f1_macro  pr_macro  roc_macro           name\n",
              "0  0.892269  0.884619  0.950481   0.969225  linsvm_C1_cal\n",
              "1  0.888318  0.878914  0.947031   0.967621       ridge_a1\n",
              "2  0.878353  0.870171  0.941549   0.963196      logreg_C2\n",
              "3  0.810845  0.773244  0.842227   0.906894        cnb_a05"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "results=[]; probas_valid={}; fitted_models={}\n",
        "for name, clf in MODELS.items():\n",
        "    print(f\"Entrenando {name} ...\")\n",
        "    clf.fit(X_train, Y_train)\n",
        "    fitted_models[name]=clf\n",
        "    p_valid = to_proba(clf, X_valid); probas_valid[name]=p_valid\n",
        "    thr = find_thresholds_per_class(Y_valid, p_valid)\n",
        "    met = metrics_from_proba(Y_valid, p_valid, thr); met.update({\"name\": name})\n",
        "    results.append({\"name\": name, \"thr\": thr, \"metrics_valid\": met})\n",
        "\n",
        "rank_df = (pd.DataFrame([r[\"metrics_valid\"] for r in results])\n",
        "           .sort_values(\"f1_macro\", ascending=False).reset_index(drop=True))\n",
        "rank_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8da7ed71",
      "metadata": {
        "id": "8da7ed71",
        "outputId": "38c72b4e-ff94-48a3-fbbb-4734f866ad2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor single: linsvm_C1_cal | test F1_macro: 0.8628\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'f1_micro': 0.8710493046776233,\n",
              " 'f1_macro': 0.8627814346493117,\n",
              " 'pr_macro': 0.9376885524844704,\n",
              " 'roc_macro': 0.9581557331766981}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_name = rank_df.loc[0,\"name\"]\n",
        "best_thr  = [r[\"thr\"] for r in results if r[\"name\"]==best_name][0]\n",
        "best_clf  = fitted_models[best_name]\n",
        "\n",
        "p_test_best = to_proba(best_clf, X_test)\n",
        "met_test_best = metrics_from_proba(Y_test, p_test_best, np.array(best_thr))\n",
        "print(\"Mejor single:\", best_name, \"| test F1_macro:\", round(met_test_best[\"f1_macro\"],4))\n",
        "met_test_best\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d9b8598",
      "metadata": {
        "id": "3d9b8598",
        "outputId": "15389aec-7b41-451c-b170-1debafa5f91e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelos en ensemble: ['logreg_C2', 'ridge_a1', 'linsvm_C1_cal', 'cnb_a05']\n",
            "Ensemble (valid): {'f1_micro': 0.8764890282131661, 'f1_macro': 0.8707740570906258, 'pr_macro': 0.9396653212221595, 'roc_macro': 0.9610729699659389}\n",
            "Ensemble (test): {'f1_micro': 0.8564263322884013, 'f1_macro': 0.845939711299291, 'pr_macro': 0.9210081362807779, 'roc_macro': 0.9486312317602891}\n"
          ]
        }
      ],
      "source": [
        "names_for_ens = list(MODELS.keys())\n",
        "print(\"Modelos en ensemble:\", names_for_ens)\n",
        "\n",
        "p_valid_ens = np.mean([probas_valid[n] for n in names_for_ens], axis=0)\n",
        "thr_ens = find_thresholds_per_class(Y_valid, p_valid_ens)\n",
        "met_ens_valid = metrics_from_proba(Y_valid, p_valid_ens, thr_ens)\n",
        "print(\"Ensemble (valid):\", met_ens_valid)\n",
        "\n",
        "p_test_ens = np.mean([to_proba(fitted_models[n], X_test) for n in names_for_ens], axis=0)\n",
        "met_ens_test = metrics_from_proba(Y_test, p_test_ens, thr_ens)\n",
        "print(\"Ensemble (test):\", met_ens_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdfea8b0",
      "metadata": {
        "id": "cdfea8b0",
        "outputId": "127bfd3c-835c-4890-844e-cbe8477fbd42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GANADOR: linsvm_C1_cal | F1_macro_test: 0.8628\n",
            "Artefactos guardados en ..\\models\\experiments\\linsvm_C1_cal\n",
            " - model.joblib OK\n",
            " - tfidf.joblib OK\n",
            " - thresholds.json OK\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json, joblib\n",
        "\n",
        "WINNER_NAME = \"ensemble_blend\" if met_ens_test[\"f1_macro\"] >= met_test_best[\"f1_macro\"] else best_name\n",
        "WINNER_THR  = thr_ens if WINNER_NAME==\"ensemble_blend\" else best_thr\n",
        "\n",
        "if WINNER_NAME==\"ensemble_blend\":\n",
        "    winner_model = {\"names\": names_for_ens, \"models\": [fitted_models[n] for n in names_for_ens]}\n",
        "    print(\"GANADOR: ensemble_blend | F1_macro_test:\", round(met_ens_test[\"f1_macro\"],4))\n",
        "else:\n",
        "    winner_model = fitted_models[WINNER_NAME]\n",
        "    print(f\"GANADOR: {WINNER_NAME} | F1_macro_test:\", round(met_test_best[\"f1_macro\"],4))\n",
        "\n",
        "WIN_DIR = Path(\"../models/experiments\")/WINNER_NAME\n",
        "WIN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "joblib.dump(winner_model, WIN_DIR/\"model.joblib\")\n",
        "joblib.dump(tfidf,       WIN_DIR/\"tfidf.joblib\")\n",
        "with open(WIN_DIR/\"thresholds.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "    json.dump({c: float(t) for c,t in zip(CLASSES, WINNER_THR)}, f, indent=2)\n",
        "\n",
        "print(\"Artefactos guardados en\", WIN_DIR)\n",
        "\n",
        "# Verificación\n",
        "for p in [\"model.joblib\",\"tfidf.joblib\",\"thresholds.json\"]:\n",
        "    fp = WIN_DIR/p\n",
        "    print(\" -\", p, \"OK\" if fp.exists() else \"FALTA\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}